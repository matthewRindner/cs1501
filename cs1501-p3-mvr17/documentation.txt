
My project was built around the use of TSTs that maximized speed over the memory. The only reasoning that my method may be more memory-intensive is the use of 4 TSTs and 2 IndexMinPQ that are that store specific attributes from a Car based on its purpose rather than creating a single large data structure that houses all the information in house. So I have the overhead of a series of smaller TSTs. 

I chose to use TSTs to store my carPQ objects due to its speed of a guaranteed runtime of worst-case N(logN) and my familiarity with the data structure. Plus I did not want to implement the remove() of a DLB which would have been marginally faster than using TSTs. I altered the TST class given from project 2 to allow differentiating between mileages and prices. My price and mileage TSTs store the Car objects and the key of car_amount, which is the ith car put in the data structure (so its indexed by size). I also Use the price and mileage IndexMinPQ to get the heap properties of each numerical attribute in constant O(1) time. These PQs store the car_amount as the value and the car's VIN number corresponding to that Car's index. Thus I can obtain all information from the series of PQs and TST using the VIN number by using the car_amount as the bridge between the PQs and the TSTs.

This is the foundation of the make_model_mileage/priceTSTs which stores all the IndexMinPQs of Car. Each IndexMinPQ is sized at 5000, which is an unreasonable number to approach since I don't think there is near that number with all the combinations of Honda and every make of the cars they have created. So this really is not an issue with runtime, but the overhead might be noticeable. I can navigate these 2 mega TSTs quickly in O(logN) + O(1) for accessing the PQs within since I can use the same indexing as in the price and mileage TSTs.

Thus getting information regarding each car with any piece of info is O(1) for lowest price and mileage, while with the parameters of make and model its max O(logN). 
